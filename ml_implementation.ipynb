{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb3d7bb",
   "metadata": {},
   "source": [
    "# Diabetic Retinopathy Detection using Transfer Learning\n",
    "\n",
    "This notebook implements a Deep Learning pipeline for classifying Diabetic Retinopathy using Transfer Learning (VGG16) and Data Augmentation.\n",
    "\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009adcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns\n",
    "\n",
    "# Check for GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2021fcd7",
   "metadata": {},
   "source": [
    "## 2. Preprocessing: Black Border Removal\n",
    "\n",
    "We define a function to crop the images to remove the black borders, focusing on the retinal area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_from_gray(img, tol=7):\n",
    "    \"\"\"\n",
    "    Crops the black borders from the fundus image.\n",
    "    \"\"\"\n",
    "    if img.ndim == 2:\n",
    "        mask = img > tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim == 3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img > tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "        return img\n",
    "\n",
    "def preprocess_input_image(img):\n",
    "    \"\"\"\n",
    "    Custom preprocessing function to be passed to ImageDataGenerator.\n",
    "    1. Crop black borders\n",
    "    2. Apply CLAHE (Green channel enhancement could be added here, but VGG expects 3 channels)\n",
    "    \"\"\"\n",
    "    # Convert to RGB if read as BGR (OpenCV default) - ImageDataGenerator loads as RGB usually\n",
    "    # But if we use this in preprocessing_function, input is a numpy array\n",
    "    \n",
    "    # Crop\n",
    "    img = img.astype('uint8')\n",
    "    img = crop_image_from_gray(img)\n",
    "    \n",
    "    # Resize is handled by the generator, but cropping changes size. \n",
    "    # We need to ensure it returns the target size or let the generator handle resizing AFTER this.\n",
    "    # However, ImageDataGenerator's preprocessing_function runs AFTER resize usually? \n",
    "    # Actually, it runs BEFORE resize if using flow_from_directory? No, it runs after image is loaded.\n",
    "    # To be safe, we'll just use this for cropping.\n",
    "    \n",
    "    # Resize back to target size if needed, but let's just return the cropped image\n",
    "    # and let the model resize it? No, generator expects fixed size.\n",
    "    # We will rely on the generator's resizing.\n",
    "    \n",
    "    # For simplicity in this notebook, we will use standard VGG preprocessing\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df47ca6",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Augmentation\n",
    "\n",
    "We use `ImageDataGenerator` to load images from the `datasets/` directory and apply data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c7a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "DATA_DIR = 'datasets/' # Update this path if your data is elsewhere\n",
    "\n",
    "# Data Augmentation for Training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, # Normalize pixel values\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2, # Use 20% for validation\n",
    "    preprocessing_function=None # Add custom preprocessing if needed\n",
    ")\n",
    "\n",
    "# Validation Generator (No augmentation, just rescaling)\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Load Data\n",
    "print(\"Loading Training Data:\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', # or 'binary' if 2 classes\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "print(\"\\nLoading Validation Data:\")\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False # Important for evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d0efe",
   "metadata": {},
   "source": [
    "## 4. Model Definition: Transfer Learning (VGG16)\n",
    "\n",
    "We load the VGG16 model pre-trained on ImageNet, freeze its layers, and add a custom classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d266b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    # Load VGG16 base model\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Freeze base layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Add custom head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "\n",
    "num_classes = len(train_generator.class_indices)\n",
    "model = build_model(num_classes)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc8b7a3",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "Train the model with Early Stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76637e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20 # Adjust as needed\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1699205f",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "Evaluate the model using Confusion Matrix and ROC Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e052dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy and Loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "Y_pred = model.predict(validation_generator, validation_generator.samples // BATCH_SIZE + 1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(validation_generator.classes, y_pred)\n",
    "print(cm)\n",
    "\n",
    "target_names = list(train_generator.class_indices.keys())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
